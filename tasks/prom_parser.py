# pars.py
import re
import json
import time
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –°–ï–õ–ï–ö–¢–û–†–Ü–í ---
MAIN_INFO_BLOCK_SELECTOR = "div[data-qaid='main_product_info']"
PRICE_SELECTOR = "div[data-qaid='product_price']"
STATUS_SELECTOR = "span[data-qaid='product_presence']"
ORDER_COUNTER_SELECTOR = "span[data-qaid='order_counter']"
RATING_SELECTOR = "div[data-qaid='product_rating']"
PAGE_NOT_FOUND_SELECTOR = "span[data-qaid='page_not_found_title']"
DELETED_WARNING_PANEL_SELECTOR = "div[data-qaid='warning_panel']"

# --- –°–õ–û–í–ù–ò–ö –°–¢–ê–¢–£–°–Ü–í ---
STATUS_MAP = {
    "–ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π": 0,
    "–ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω": 0,
    "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ": 1,
    "–í –Ω–∞–ª–∏—á–∏–∏": 1,
    "–ì–æ—Ç–æ–≤–æ –¥–æ –≤—ñ–¥–ø—Ä–∞–≤–∫–∏": 2,
    "–ì–æ—Ç–æ–≤–æ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ": 2,
    "–ü—ñ–¥ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è": 3,
    "–ü–æ–¥ –∑–∞–∫–∞–∑": 3,
}


def _extract_number(text: str) -> int | None:
    """–î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏—Ç—è–≥–∞–Ω–Ω—è –ø–µ—Ä—à–æ–≥–æ —á–∏—Å–ª–∞ –∑ —Ç–µ–∫—Å—Ç—É."""
    if not text:
        return None
    match = re.search(r"(\d+)", str(text))
    return int(match.group(1)) if match else None


def parse_product_data(products_to_scrape: list, headless_mode: bool = True) -> str:
    """
    –ê–≤—Ç–æ–Ω–æ–º–Ω–∏–π –º–æ–¥—É–ª—å –ø–∞—Ä—Å–∏–Ω–≥—É.
    –ü—Ä–∏–π–º–∞—î —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤, –æ–±—Ä–æ–±–ª—è—î —ó—Ö —ñ –ø–æ–≤–µ—Ä—Ç–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON.
    """
    scraped_data = []
    driver = None

    try:
        # --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–∞–ø—É—Å–∫ Selenium (undetected-chromedriver) ---
        options = uc.ChromeOptions()
        options.add_argument("--headless")

        options.add_argument("--window-size=1920,1080")
        options.add_argument("--no-sandbox")

        driver = uc.Chrome(options=options)

        # --- –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –æ–±—Ä–æ–±–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤ ---
        for product in products_to_scrape:
            product_id = product.get("product_id")
            product_url = product.get("url")

            if not all([product_id, product_url]):
                continue

            driver.get(product_url)

            daily_data = {
                "product_id": product_id,
                "status_id": None,
                "price": None,
                "order_quantity": None,
                "rating": None,
            }

            try:
                driver.find_element(By.CSS_SELECTOR, DELETED_WARNING_PANEL_SELECTOR)
                daily_data["status_id"] = 4
                scraped_data.append(daily_data)
                continue
            except NoSuchElementException:
                pass

            try:
                wait = WebDriverWait(driver, 5)
                status_element = wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, STATUS_SELECTOR))
                )
                status_text = status_element.text.lower()
                for text_key, status_id in STATUS_MAP.items():
                    if text_key in status_text:
                        daily_data["status_id"] = status_id
                        break
            except TimeoutException:
                try:
                    driver.find_element(By.CSS_SELECTOR, PAGE_NOT_FOUND_SELECTOR)
                    daily_data["status_id"] = 0
                except NoSuchElementException:
                    pass

            try:
                main_info_block = driver.find_element(
                    By.CSS_SELECTOR, MAIN_INFO_BLOCK_SELECTOR
                )
                try:
                    price_element = main_info_block.find_element(
                        By.CSS_SELECTOR, PRICE_SELECTOR
                    )
                    daily_data["price"] = float(
                        price_element.get_attribute("data-qaprice")
                    )
                except (NoSuchElementException, TypeError, ValueError):
                    pass

                try:
                    orders_text = main_info_block.find_element(
                        By.CSS_SELECTOR, ORDER_COUNTER_SELECTOR
                    ).text
                    daily_data["order_quantity"] = _extract_number(orders_text)
                except NoSuchElementException:
                    pass
            except NoSuchElementException:
                pass

            try:
                rating_element = driver.find_element(By.CSS_SELECTOR, RATING_SELECTOR)
                daily_data["rating"] = float(
                    rating_element.get_attribute("data-qarating")
                )
            except (NoSuchElementException, TypeError, ValueError):
                pass

            if daily_data.get("status_id") is None:
                daily_data["status_id"] = 5

            scraped_data.append(daily_data)
            time.sleep(1)  # –ü–∞—É–∑–∞ –≤ 1 —Å–µ–∫—É–Ω–¥—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏

        # –Ø–∫—â–æ —Ü–∏–∫–ª –∑–∞–≤–µ—Ä—à–∏–≤—Å—è –±–µ–∑ –ø–æ–º–∏–ª–æ–∫, –≥–æ—Ç—É—î–º–æ —É—Å–ø—ñ—à–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        success_result = {"status": "success", "data": scraped_data}
        return json.dumps(success_result, indent=4, ensure_ascii=False)

    except Exception as e:
        # –£ —Ä–∞–∑—ñ –±—É–¥—å-—è–∫–æ—ó –∫—Ä–∏—Ç–∏—á–Ω–æ—ó –ø–æ–º–∏–ª–∫–∏, —Ñ–æ—Ä–º—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–æ –ø–æ–º–∏–ª–∫—É
        error_result = {
            "status": "error",
            "message": f"üî• –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ä–æ–±–æ—Ç–∏ –ø–∞—Ä—Å–µ—Ä–∞: {str(e)}",
        }
        return json.dumps(error_result, indent=4, ensure_ascii=False)

    finally:
        # –ë–ª–æ–∫ finally –≤–∏–∫–æ–Ω–∞—î—Ç—å—Å—è –≤ –±—É–¥—å-—è–∫–æ–º—É –≤–∏–ø–∞–¥–∫—É –¥–ª—è –∑–∞–∫—Ä–∏—Ç—Ç—è –¥—Ä–∞–π–≤–µ—Ä–∞
        if driver:
            driver.quit()
